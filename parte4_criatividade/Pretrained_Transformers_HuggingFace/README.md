
# README

Este repositório contém três notebooks que exploram diferentes modelos de geração de linguagem baseados em redes neurais profundas. Através desses exemplos, buscamos entender os princípios fundamentais que sustentam modelos maiores e mais complexos como o ChatGPT. Estamos utilizando a biblioteca e os recursos da plataforma HuggingFace para facilitar o acesso e a manipulação desses modelos.

## Notebooks Disponíveis

1. **EleutherAI_gpt-neo-2.7B.ipynb**
   - **Descrição**: Este notebook utiliza o modelo GPT-Neo com 2.7 bilhões de parâmetros, desenvolvido pela EleutherAI. O GPT-Neo é uma implementação aberta e acessível de um modelo de linguagem semelhante ao GPT-3 da OpenAI. Ele é capaz de gerar texto coerente e relevante baseado em entradas fornecidas pelo usuário.
   - **Objetivo**: Demonstrar a capacidade do GPT-Neo de gerar texto de alta qualidade e explorar as aplicações práticas deste modelo em tarefas de processamento de linguagem natural (NLP).

2. **GPT2.ipynb**
   - **Descrição**: Este notebook explora o modelo GPT-2, um dos primeiros modelos de grande escala desenvolvido pela OpenAI. Com diferentes tamanhos disponíveis (124M, 355M, 774M, 1.5B), o GPT-2 revolucionou o campo de NLP ao mostrar que modelos treinados com grandes quantidades de dados podem gerar texto com um nível de coerência impressionante.
   - **Objetivo**: Investigar as capacidades do GPT-2 em gerar texto e compreender como o aumento no número de parâmetros impacta a performance do modelo.

3. **pierrreguillou_gpt2-small-portuguese.ipynb**
   - **Descrição**: Este notebook utiliza uma versão adaptada do GPT-2 para a língua portuguesa, desenvolvida por Pierre Guillou. Esta adaptação permite que o modelo gere texto em português com uma qualidade comparável aos modelos em inglês, demonstrando a versatilidade e a aplicabilidade de tais modelos em diferentes idiomas.
   - **Objetivo**: Avaliar a performance do GPT-2 em português e entender os desafios e benefícios de adaptar modelos de linguagem para diferentes idiomas.

## Recursos Utilizados

- **Biblioteca HuggingFace**: Estamos utilizando a biblioteca `transformers` da HuggingFace, que fornece uma interface simples e poderosa para o uso de modelos de linguagem pré-treinados. Esta biblioteca facilita o acesso a uma ampla gama de modelos e nos permite realizar ajustes finos e avaliações detalhadas de suas capacidades.
- **Plataforma HuggingFace**: Além da biblioteca, a plataforma HuggingFace oferece uma infraestrutura robusta para o compartilhamento e a execução de modelos, permitindo a colaboração e a reprodução de experimentos de maneira eficiente.

## Objetivos Educacionais

O principal objetivo destes notebooks é fornecer uma base sólida sobre como os modelos de geração de linguagem funcionam e como eles podem ser aplicados em diversos contextos. Ao explorar estes exemplos, os alunos serão capazes de:
- Compreender os conceitos básicos de redes neurais profundas aplicadas à geração de linguagem.
- Avaliar as diferenças entre modelos de diferentes tamanhos e complexidades.
- Aplicar modelos pré-treinados em tarefas práticas de NLP.
- Adaptar e ajustar modelos de linguagem para diferentes idiomas e contextos.

Estes conhecimentos são fundamentais para o entendimento e a utilização de modelos maiores, como o ChatGPT, permitindo que os alunos avancem em suas pesquisas e aplicações práticas no campo da inteligência artificial.

Esperamos que este repositório seja uma ferramenta valiosa no aprendizado sobre modelos de linguagem e inspire a exploração contínua neste campo dinâmico e em rápida evolução.
