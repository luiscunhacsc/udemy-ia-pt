{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luiscunhacsc/udemy-ia-pt/blob/main/parte3_linguagem/DL_with_Python_Chollet_Translation_Transformer/parte3_exemplo01_translation_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXEeAxR92JRs"
      },
      "source": [
        "## This example follows closely the code in the recommended book: <br> Chollet, F. (2021). Deep Learning with Python, 2nd Ed. Manning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-1IkUqD2JRt"
      },
      "source": [
        "### A machine translation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCMxGPEy2JRt",
        "outputId": "a9f83cd9-44dd-4ad8-f3d5-2a3b1cfedbb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/device:CPU:0', '/device:GPU:0']\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_devices():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos]\n",
        "\n",
        "print(get_available_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJzhDS2F2JRt",
        "outputId": "b618b17b-ccc4-4538-b070-bb21b0710fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-27 15:54:50--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.128.207, 74.125.143.207, 173.194.69.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.128.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   2.52M  3.89MB/s    in 0.6s    \n",
            "\n",
            "2023-11-27 15:54:50 (3.89 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiXOFDqB2JRu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q2cByb12JRu"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file, encoding='utf-8') as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj3gypws2JRu",
        "outputId": "7fbb18e6-a112-46bb-bfb9-2230a4838131"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(\"Nobody's blaming you.\", '[start] Nadie te está culpando. [end]')\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaJ2yJzn2JRu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZFxQFXX2JRu"
      },
      "source": [
        "**Vectorizing the English and Spanish text pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bae_Lblw2JRv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCFHh5i2JRv"
      },
      "source": [
        "**Preparing datasets for the translation task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDkVkVhj2JRv"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOpbP02l2JRv",
        "outputId": "499a3138-c48f-42c0-c108-81e72d0c4581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMaUlvbw2JRv"
      },
      "source": [
        "### Sequence-to-sequence learning with Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULqtZRuK2JRw"
      },
      "source": [
        "#### The Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE3QOHf32JRw"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpr5HOqb2JRw"
      },
      "source": [
        "#### The Transformer decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVNh2a3V2JRw"
      },
      "source": [
        "**The `TransformerDecoder`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xXGqJTg2JRw"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJVCotEy2JRw"
      },
      "source": [
        "#### Putting it all together: A Transformer for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3SzEa_2JRx"
      },
      "source": [
        "**PositionalEmbedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MENKAoh-2JRx"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0XY6jC22JRx"
      },
      "source": [
        "**End-to-end Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywh5sTaX2JRx"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "transformer.compile(\n",
        "    optimizer='adam',  # You can choose optimizers like 'adam', 'sgd', etc.\n",
        "    loss='sparse_categorical_crossentropy',  # This is a common loss for classification problems\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcEYG5OqfPD3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72y_x0uM2JRx"
      },
      "source": [
        "**Training the sequence-to-sequence Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30f0e9d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a callback to save the model when the accuracy improves\n",
        "class SaveBestModel(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_pairs):\n",
        "        super(SaveBestModel, self).__init__()\n",
        "        self.best_accuracy = 0.0\n",
        "        self.test_pairs = test_pairs\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_accuracy = logs.get('val_accuracy')\n",
        "        if val_accuracy is not None and val_accuracy > self.best_accuracy:\n",
        "            self.best_accuracy = val_accuracy\n",
        "            print(\"New best model found! Saving to 'best_model_translation_eng_spa.h5'\")\n",
        "            self.model.save(\"best_model_translation_eng_spa.h5\")\n",
        "\n",
        "            # Print 10 random translation examples\n",
        "            print(\"10 random examples of translation:\")\n",
        "            test_eng_texts = [pair[0] for pair in self.test_pairs]\n",
        "            for _ in range(10):\n",
        "                input_sentence = random.choice(test_eng_texts)\n",
        "                print(\"-\")\n",
        "                print(input_sentence)\n",
        "                print(decode_sequence(input_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1C5pzID2JRx",
        "outputId": "8c21b201-ce2e-45b8-e5f9-71b80764b0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 3.4509 - accuracy: 0.4763New best model found! Saving to 'best_model_translation_eng_spa.h5'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 random examples of translation:\n",
            "-\n",
            "That's where we'll go.\n",
            "[start] eso está iremos a ir [end]\n",
            "-\n",
            "Tom is memorizing a poem.\n",
            "[start] tom es un error [end]\n",
            "-\n",
            "I like to learn new things.\n",
            "[start] me gusta aprender las cosas de cosas [end]\n",
            "-\n",
            "All my friends and family are dead.\n",
            "[start] todos mis amigos y familia son [UNK] [end]\n",
            "-\n",
            "I never agree with him.\n",
            "[start] nunca me siento con él [end]\n",
            "-\n",
            "Do you have a special menu for vegetarians?\n",
            "[start] tienes un papel para [UNK] por los daños [end]\n",
            "-\n",
            "We will be together forever.\n",
            "[start] nos vamos a ir a las cuatro días [end]\n",
            "-\n",
            "Take off your clothes.\n",
            "[start] toma tu ropa [end]\n",
            "-\n",
            "He cannot play guitar.\n",
            "[start] no puede tocar guitarra [end]\n",
            "-\n",
            "What're you all dressed up for?\n",
            "[start] qué estás todo por qué por cualquier cosa [end]\n",
            "1302/1302 [==============================] - 129s 88ms/step - loss: 3.4509 - accuracy: 0.4763 - val_loss: 2.3192 - val_accuracy: 0.5936\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 2.2040 - accuracy: 0.6186New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "Tom told Mary.\n",
            "[start] tom le dijo a mary [end]\n",
            "-\n",
            "Why did you decide to buy this house?\n",
            "[start] por qué decidiste por favor [end]\n",
            "-\n",
            "She lost her temper with me.\n",
            "[start] ella perdió la paciencia conmigo [end]\n",
            "-\n",
            "Tom climbed down from the tree.\n",
            "[start] tom bajó del árbol [end]\n",
            "-\n",
            "What do you do before breakfast?\n",
            "[start] qué haces antes de antes [end]\n",
            "-\n",
            "I've got something to do.\n",
            "[start] tengo algo que hacer [end]\n",
            "-\n",
            "You should relax a little.\n",
            "[start] deberías [UNK] un poco [end]\n",
            "-\n",
            "You never believed me.\n",
            "[start] nunca me has dicho [end]\n",
            "-\n",
            "I'm sorry, there's nothing I can do.\n",
            "[start] lo siento no hay nada que pueda hacer [end]\n",
            "-\n",
            "That's where we'll go.\n",
            "[start] eso está aquí donde vamos [end]\n",
            "1302/1302 [==============================] - 103s 79ms/step - loss: 2.2040 - accuracy: 0.6186 - val_loss: 1.8909 - val_accuracy: 0.6517\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.7443 - accuracy: 0.6732New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "Where do the airport buses leave from?\n",
            "[start] dónde están el aeropuerto de autobuses [end]\n",
            "-\n",
            "It isn't fair.\n",
            "[start] no es justo [end]\n",
            "-\n",
            "Who would you rather go out with, Tom or John?\n",
            "[start] quién harías contigo [end]\n",
            "-\n",
            "Tom is being kind of devious, isn't he?\n",
            "[start] tom es estar tan amable de no es [end]\n",
            "-\n",
            "Why do you want to hurt Tom?\n",
            "[start] por qué quieres lastimar a tom [end]\n",
            "-\n",
            "She smiled sadly.\n",
            "[start] ella le sonrió [end]\n",
            "-\n",
            "She approved of the wedding.\n",
            "[start] ella aprobó la boda [end]\n",
            "-\n",
            "I think the story is true.\n",
            "[start] creo que la historia es verdad [end]\n",
            "-\n",
            "I'll come to your place.\n",
            "[start] voy a su lugar [end]\n",
            "-\n",
            "They gave a big party for me.\n",
            "[start] ellos se compraron una gran fiesta para mí [end]\n",
            "1302/1302 [==============================] - 103s 79ms/step - loss: 1.7443 - accuracy: 0.6732 - val_loss: 1.7030 - val_accuracy: 0.6771\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.4733 - accuracy: 0.7065New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "Tom always wears sunglasses and a trench coat.\n",
            "[start] tom siempre lleva lentes y lentes de contacto con su abrigo [end]\n",
            "-\n",
            "We never go to bed before midnight.\n",
            "[start] nunca nos llegamos a la cama antes de medianoche [end]\n",
            "-\n",
            "It takes two to tango.\n",
            "[start] se toma dos gatos [end]\n",
            "-\n",
            "It'll take me a long time to do all the things I want to do.\n",
            "[start] me tomará mucho tiempo para hacer todo lo que yo y yo quiero hacer [end]\n",
            "-\n",
            "We all make mistakes.\n",
            "[start] todos tenemos errores [end]\n",
            "-\n",
            "Tom appealed for help.\n",
            "[start] tom [UNK] para ayudar [end]\n",
            "-\n",
            "He's an accredited representative of the Canadian government.\n",
            "[start] Él es un representante de nueva caja canadiense [end]\n",
            "-\n",
            "He earns 300,000 yen a month.\n",
            "[start] Él gana [UNK] un mes [end]\n",
            "-\n",
            "When Tom stopped for a stop sign, his engine stalled.\n",
            "[start] cuando tom dejó de firmar un cumplido las vidas [UNK] [end]\n",
            "-\n",
            "It snowed yesterday.\n",
            "[start] ayer llovió [end]\n",
            "1302/1302 [==============================] - 101s 78ms/step - loss: 1.4733 - accuracy: 0.7065 - val_loss: 1.6339 - val_accuracy: 0.6880\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.2903 - accuracy: 0.7306New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "Unless it rains, I will go, too.\n",
            "[start] a veces que voy a ir también [end]\n",
            "-\n",
            "Tom left town.\n",
            "[start] tom se fue de la ciudad [end]\n",
            "-\n",
            "He had gray hair.\n",
            "[start] Él tuvo pelo [UNK] [end]\n",
            "-\n",
            "I should be making lunch.\n",
            "[start] debería estar haciendo almorzar [end]\n",
            "-\n",
            "Can you give me some money?\n",
            "[start] me puedes dar algo de dinero [end]\n",
            "-\n",
            "Don't back up. There's a tree behind you.\n",
            "[start] no te [UNK] un árbol [end]\n",
            "-\n",
            "I'm in a pickle.\n",
            "[start] estoy en apuros [end]\n",
            "-\n",
            "It's necessary for her to go herself.\n",
            "[start] es necesario que ella se vaya sola [end]\n",
            "-\n",
            "After the accident, the police told the crowd to keep back.\n",
            "[start] después del accidente le dijeron la policía a la escena del espectáculo [end]\n",
            "-\n",
            "He is unquestionably the oldest man in the village.\n",
            "[start] Él es el hombre de la persona más grande del pueblo [end]\n",
            "1302/1302 [==============================] - 99s 76ms/step - loss: 1.2903 - accuracy: 0.7306 - val_loss: 1.6200 - val_accuracy: 0.6930\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.1543 - accuracy: 0.7502New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "Tom never said one word.\n",
            "[start] tom nunca dijo una palabra [end]\n",
            "-\n",
            "I think we should do some more.\n",
            "[start] creo que deberíamos hacer más [end]\n",
            "-\n",
            "I didn't get along with her.\n",
            "[start] no me fui a llevarse bien con ella [end]\n",
            "-\n",
            "Tom saw Mary and John kissing.\n",
            "[start] tom vio a mary y john empezaron a besar [end]\n",
            "-\n",
            "Tom closed the door quietly and tiptoed into the room.\n",
            "[start] tom cerró la puerta y entró en la habitación [end]\n",
            "-\n",
            "Please give me a cup of water.\n",
            "[start] por favor dame una taza de agua [end]\n",
            "-\n",
            "Tom wondered if what Mary said was true.\n",
            "[start] tom se preguntaba qué dijo lo que mary dijo [end]\n",
            "-\n",
            "I am the same age.\n",
            "[start] yo soy el mismo años [end]\n",
            "-\n",
            "I like this saying.\n",
            "[start] me gusta esto [end]\n",
            "-\n",
            "My mother set the table.\n",
            "[start] mi madre le ha dejado la mesa [end]\n",
            "1302/1302 [==============================] - 102s 79ms/step - loss: 1.1543 - accuracy: 0.7502 - val_loss: 1.5794 - val_accuracy: 0.7027\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 1.0440 - accuracy: 0.7674New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "Tom and I work at the same hospital.\n",
            "[start] tom y yo trabajamos al el mismo hospital [end]\n",
            "-\n",
            "Your chair is identical to mine.\n",
            "[start] tu silla es que está [UNK] a mi padre [end]\n",
            "-\n",
            "Come back soon.\n",
            "[start] vuelve pronto [end]\n",
            "-\n",
            "Camels are the ships of the deserts.\n",
            "[start] los camellos son los productos de [UNK] [end]\n",
            "-\n",
            "She cut her hand with a knife.\n",
            "[start] ella le cortó la mano con un cuchillo [end]\n",
            "-\n",
            "Tom has to go to the hospital.\n",
            "[start] tom tiene que ir al hospital [end]\n",
            "-\n",
            "Getting your message across is much more important than trying to say it exactly like a native speaker would say it.\n",
            "[start] [UNK] tu mensaje con él más importante que decir que lo diga que es que decir una lengua [UNK] de\n",
            "-\n",
            "I think everybody is ready.\n",
            "[start] pienso que todos están listos [end]\n",
            "-\n",
            "What more do you want, Tom?\n",
            "[start] qué más quieres a tom [end]\n",
            "-\n",
            "I hope Tom doesn't get sick.\n",
            "[start] espero que tom no esté enfermo [end]\n",
            "1302/1302 [==============================] - 104s 80ms/step - loss: 1.0440 - accuracy: 0.7674 - val_loss: 1.5785 - val_accuracy: 0.7033\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.9565 - accuracy: 0.7817New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "I play tennis in the park on Sunday.\n",
            "[start] juego al tenis en el parque los domingos [end]\n",
            "-\n",
            "He is bankrupt.\n",
            "[start] Él es la bancarrota [end]\n",
            "-\n",
            "I lost the game.\n",
            "[start] perdí el juego [end]\n",
            "-\n",
            "You're too idealistic.\n",
            "[start] eres demasiado [UNK] [end]\n",
            "-\n",
            "It was fine all day.\n",
            "[start] todo fue bien el día [end]\n",
            "-\n",
            "Tom hasn't said a thing all day.\n",
            "[start] tom no ha dicho algo todo el día [end]\n",
            "-\n",
            "It's our destiny.\n",
            "[start] es nuestro destino [end]\n",
            "-\n",
            "Selena Gomez has just released her second album.\n",
            "[start] selena gomez acaba de [UNK] su plan [end]\n",
            "-\n",
            "I was born in 1988 in York.\n",
            "[start] nací en 1988 [end]\n",
            "-\n",
            "Let's get this done and get out of here.\n",
            "[start] vamos a hacer esto y ya se quedaron aquí [end]\n",
            "1302/1302 [==============================] - 102s 78ms/step - loss: 0.9565 - accuracy: 0.7817 - val_loss: 1.5906 - val_accuracy: 0.7063\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8817 - accuracy: 0.7945New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "I wouldn't blame them.\n",
            "[start] no los [UNK] [end]\n",
            "-\n",
            "You can't give up now.\n",
            "[start] no puedes dar ahora [end]\n",
            "-\n",
            "Tom looked over the fence.\n",
            "[start] tom miró a la cerca [end]\n",
            "-\n",
            "We went down the river in a small boat.\n",
            "[start] fuimos al quinto río en un bote [end]\n",
            "-\n",
            "Tom became scared as soon as he saw the robber's knife.\n",
            "[start] tom se durmió tan pronto como vio el cuchillo cuchillo [end]\n",
            "-\n",
            "When you travel abroad, you usually need a passport.\n",
            "[start] cuando te vas al extranjero alguna prueba [end]\n",
            "-\n",
            "You're very clever.\n",
            "[start] eres muy listo [end]\n",
            "-\n",
            "We adopted Tom.\n",
            "[start] [UNK] a tom [end]\n",
            "-\n",
            "Tom doesn't know anything about Boston.\n",
            "[start] tom no sabe nada acerca de boston [end]\n",
            "-\n",
            "She writes with her left hand.\n",
            "[start] ella escribe con la mano izquierda [end]\n",
            "1302/1302 [==============================] - 102s 78ms/step - loss: 0.8817 - accuracy: 0.7945 - val_loss: 1.6329 - val_accuracy: 0.7078\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.8241 - accuracy: 0.8049New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "I took my shoes off and threw them out the window.\n",
            "[start] tomé mi calzado y [UNK] el arrojó afuera [end]\n",
            "-\n",
            "I thought you'd be full after eating that big steak.\n",
            "[start] pensé que estabas comiendo tom después de comer demasiado ardiente [end]\n",
            "-\n",
            "He sometimes comes home late.\n",
            "[start] Él a veces llega tarde a casa [end]\n",
            "-\n",
            "What do you want now?\n",
            "[start] qué quieres ahora [end]\n",
            "-\n",
            "I can't believe we're here.\n",
            "[start] no puedo creer que estaban aquí [end]\n",
            "-\n",
            "Do you have a shoe box where I can put these things?\n",
            "[start] tienes una chaqueta de zapatos en la que puedo poner estas cosas [end]\n",
            "-\n",
            "I picked up some French.\n",
            "[start] yo tomé algo de francés [end]\n",
            "-\n",
            "Careless driving causes accidents.\n",
            "[start] el conducir debido a la sala fracasaron [UNK] [end]\n",
            "-\n",
            "I cannot disclose any information about the informant.\n",
            "[start] no puedo revelar ninguna información acerca de los [UNK] del lunes [end]\n",
            "-\n",
            "I have problems with my wife, too.\n",
            "[start] tengo problemas con mi mujer [end]\n",
            "1302/1302 [==============================] - 102s 79ms/step - loss: 0.8241 - accuracy: 0.8049 - val_loss: 1.6406 - val_accuracy: 0.7089\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.7732 - accuracy: 0.8149New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "I believe love exists.\n",
            "[start] creo que existe vida [end]\n",
            "-\n",
            "There have been hitches.\n",
            "[start] había estado [UNK] [end]\n",
            "-\n",
            "How do you feel today?\n",
            "[start] cómo te sientes hoy [end]\n",
            "-\n",
            "I've got no time for that now.\n",
            "[start] no tengo tiempo para esto [end]\n",
            "-\n",
            "I don't dare talk to her.\n",
            "[start] no me soporto con ella [end]\n",
            "-\n",
            "This old building has seen better days.\n",
            "[start] este viejo edificio ha visto días mejor [end]\n",
            "-\n",
            "Aren't you a little young?\n",
            "[start] no eres un poco joven [end]\n",
            "-\n",
            "Tom will have to wait.\n",
            "[start] tom tendrá que esperar [end]\n",
            "-\n",
            "I just want to get a little fresh air.\n",
            "[start] solo quiero conseguir un poco de aire fresco [end]\n",
            "-\n",
            "You despise me, don't you?\n",
            "[start] no me [UNK] verdad [end]\n",
            "1302/1302 [==============================] - 98s 75ms/step - loss: 0.7732 - accuracy: 0.8149 - val_loss: 1.6614 - val_accuracy: 0.7098\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.7263 - accuracy: 0.8235 - val_loss: 1.7002 - val_accuracy: 0.7095\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.8310New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "We can't live without oxygen.\n",
            "[start] no podemos vivir sin el oxígeno [end]\n",
            "-\n",
            "I was certain that you'd come.\n",
            "[start] estaba seguro de que vinieras [end]\n",
            "-\n",
            "The people next door were annoyed with us for making so much noise last night.\n",
            "[start] la gente que se veía muy [UNK] por la noche así que estaba enfadada [end]\n",
            "-\n",
            "My grandfather is fond of taking a walk early in the morning.\n",
            "[start] a mi abuelo le gusta un poco temprano a partir de la mañana [end]\n",
            "-\n",
            "Call me once in a while.\n",
            "[start] llámame cuando [end]\n",
            "-\n",
            "Tom questioned Mary.\n",
            "[start] tom [UNK] a mary [end]\n",
            "-\n",
            "You can have three guesses.\n",
            "[start] puedes intentar adivinarlo tres veces [end]\n",
            "-\n",
            "Keep them.\n",
            "[start] [UNK] [end]\n",
            "-\n",
            "Tom picked up something off the table.\n",
            "[start] tom recogió algo sobre la mesa [end]\n",
            "-\n",
            "We should work faster.\n",
            "[start] deberíamos trabajar más rápido [end]\n",
            "1302/1302 [==============================] - 99s 76ms/step - loss: 0.6872 - accuracy: 0.8310 - val_loss: 1.7194 - val_accuracy: 0.7104\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 93s 72ms/step - loss: 0.6541 - accuracy: 0.8376 - val_loss: 1.7394 - val_accuracy: 0.7097\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.6186 - accuracy: 0.8445 - val_loss: 1.8006 - val_accuracy: 0.7096\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.5915 - accuracy: 0.8498 - val_loss: 1.8130 - val_accuracy: 0.7085\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.5642 - accuracy: 0.8561New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "The two men fought for a long time.\n",
            "[start] los dos hombres lucharon por una eternidad [end]\n",
            "-\n",
            "It took me several hours to write it.\n",
            "[start] me tomó varias horas por escribirlo [end]\n",
            "-\n",
            "The crowd is going nuts.\n",
            "[start] la multitud se va a crecer [end]\n",
            "-\n",
            "Some of the books that he has are English novels.\n",
            "[start] algunas de los libros que tiene mucha novelas [end]\n",
            "-\n",
            "I can feel it in my bones.\n",
            "[start] puedo sentirlo en mi [UNK] [end]\n",
            "-\n",
            "She abandoned her children.\n",
            "[start] ella abandonó a los hijos [end]\n",
            "-\n",
            "The train was delayed for an hour.\n",
            "[start] el tren se retrasó por una hora [end]\n",
            "-\n",
            "Why don't you seem as happy as I am?\n",
            "[start] por qué no te parece feliz yo estoy tan feliz [end]\n",
            "-\n",
            "Do you want to do it?\n",
            "[start] quieres hacerlo [end]\n",
            "-\n",
            "I hope I haven't woken you up.\n",
            "[start] espero que no te he despertado [end]\n",
            "1302/1302 [==============================] - 102s 78ms/step - loss: 0.5642 - accuracy: 0.8561 - val_loss: 1.8221 - val_accuracy: 0.7105\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.5420 - accuracy: 0.8604 - val_loss: 1.8767 - val_accuracy: 0.7073\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - ETA: 0s - loss: 0.5170 - accuracy: 0.8658New best model found! Saving to 'best_model_translation_eng_spa.h5'\n",
            "10 random examples of translation:\n",
            "-\n",
            "At noon, I have lunch with my classmates.\n",
            "[start] a mediodía los mediodía con mis compañeros [end]\n",
            "-\n",
            "Call a doctor.\n",
            "[start] llama a un médico [end]\n",
            "-\n",
            "This must be yours.\n",
            "[start] esto debe ser vuestro [end]\n",
            "-\n",
            "I demand to know what's going on here.\n",
            "[start] [UNK] [end]\n",
            "-\n",
            "Thanks.\n",
            "[start] gracias [end]\n",
            "-\n",
            "It didn't surprise me at all that Tom got arrested from drunken driving.\n",
            "[start] no me sorprendió a toda la que tom [UNK] de conducir bajo la finca de muy conduciendo con mi energía\n",
            "-\n",
            "Have you ever dyed your hair?\n",
            "[start] alguna vez te has teñido el pelo [end]\n",
            "-\n",
            "You have to cross the ocean to get to America.\n",
            "[start] tienes que pagar la tierra en estados unidos [end]\n",
            "-\n",
            "I have a return ticket to Tokyo.\n",
            "[start] tengo un pasaje de vuelta a tokio [end]\n",
            "-\n",
            "Tom invited Mary out to lunch.\n",
            "[start] tom invitó a mary a almorzar [end]\n",
            "1302/1302 [==============================] - 102s 79ms/step - loss: 0.5170 - accuracy: 0.8658 - val_loss: 1.8738 - val_accuracy: 0.7114\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 93s 71ms/step - loss: 0.4968 - accuracy: 0.8705 - val_loss: 1.9159 - val_accuracy: 0.7094\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 93s 71ms/step - loss: 0.4793 - accuracy: 0.8740 - val_loss: 1.9533 - val_accuracy: 0.7085\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 93s 71ms/step - loss: 0.4582 - accuracy: 0.8785 - val_loss: 1.9666 - val_accuracy: 0.7085\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 93s 71ms/step - loss: 0.4453 - accuracy: 0.8816 - val_loss: 1.9675 - val_accuracy: 0.7068\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 93s 71ms/step - loss: 0.4260 - accuracy: 0.8863 - val_loss: 2.0291 - val_accuracy: 0.7069\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 92s 70ms/step - loss: 0.4166 - accuracy: 0.8881 - val_loss: 2.0217 - val_accuracy: 0.7075\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 92s 70ms/step - loss: 0.4013 - accuracy: 0.8917 - val_loss: 2.0563 - val_accuracy: 0.7105\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.3898 - accuracy: 0.8944 - val_loss: 2.0712 - val_accuracy: 0.7091\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 92s 71ms/step - loss: 0.3753 - accuracy: 0.8979 - val_loss: 2.0549 - val_accuracy: 0.7100\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 93s 72ms/step - loss: 0.3657 - accuracy: 0.9001 - val_loss: 2.0838 - val_accuracy: 0.7090\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 93s 71ms/step - loss: 0.3565 - accuracy: 0.9023 - val_loss: 2.1228 - val_accuracy: 0.7095\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b60ccf8b2b0>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Train the model using the new callback\n",
        "save_best_model_callback = SaveBestModel(test_pairs)\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds,\n",
        "                callbacks=[save_best_model_callback])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3_15Lug2JRx"
      },
      "source": [
        "**Translating new sentences with our Transformer model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_PklCp62JRx",
        "outputId": "f3058ebb-0161-42c2-8476-987f8e33b545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Final Results:\n",
            "-\n",
            "She doesn't live with him.\n",
            "[start] ella no vive con él [end]\n",
            "-\n",
            "I forget your telephone number.\n",
            "[start] olvidé tu número de teléfono [end]\n",
            "-\n",
            "I have a dentist appointment.\n",
            "[start] tengo una cita con dentista [end]\n",
            "-\n",
            "My dress was ruined when it came back from the cleaner's.\n",
            "[start] mi vestido ha arruinado cuando volvió de por la [end]\n",
            "-\n",
            "How long have you been staying in Osaka?\n",
            "[start] cuánto tiempo has estado durmiendo en osaka [end]\n",
            "-\n",
            "I don't know the reason why he went there.\n",
            "[start] no sé por qué él fue ahí [end]\n",
            "-\n",
            "He lent me two books.\n",
            "[start] Él me dio guantes dos libros [end]\n",
            "-\n",
            "Those are our orders.\n",
            "[start] esas son nuestras órdenes [end]\n",
            "-\n",
            "We never go to bed before midnight.\n",
            "[start] nunca vamos a la cama antes de medianoche [end]\n",
            "-\n",
            "Don't you feel cold?\n",
            "[start] no te sientes helado [end]\n",
            "-\n",
            "I want you to know that I love you.\n",
            "[start] quiero que sepas que te amo [end]\n",
            "-\n",
            "I always wondered if you'd come back.\n",
            "[start] siempre me preguntaba si has vuelto [end]\n",
            "-\n",
            "Japan is very different from what it was fifty years ago.\n",
            "[start] japón es muy diferente de lo que era hace 50 años [end]\n",
            "-\n",
            "Have both of you already eaten?\n",
            "[start] ya te has comido aquí [end]\n",
            "-\n",
            "We're in the well.\n",
            "[start] estamos en el pozo [end]\n",
            "-\n",
            "Aren't you on duty tonight?\n",
            "[start] no estás de esta noche [end]\n",
            "-\n",
            "Wake Tom up.\n",
            "[start] [UNK] a tom [end]\n",
            "-\n",
            "I have no time tomorrow.\n",
            "[start] no tengo tiempo mañana [end]\n",
            "-\n",
            "What were they talking about?\n",
            "[start] de qué estaban hablando [end]\n",
            "-\n",
            "I bought a set of table linen.\n",
            "[start] compré una firma de pañuelos [end]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\");\n",
        "print(\"Final Results:\")\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
